{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYeU1RuncYji"
   },
   "source": [
    "# RAT-Inspired ColQwen\n",
    "\n",
    "This notebook implements a **RAT-inspired** methodology coupled with **ColQwen**, a VLM-based model for information retrieval. This pipeline draws from the principles of **Retrieval-Augmented Thoughts (RAT)** and it adapts these ideas to fit a more focused document analysis workflow, balancing iterative reasoning and multimodal document processing.\n",
    "\n",
    "## Key Components:\n",
    "1. **RAT-Inspired pipeline**:\n",
    "    - **Iterative Chain-of-Thought (CoT) Refinement**:\n",
    "      - Starts with a zero-shot CoT prompt to draft the initial reasoning process.\n",
    "      - Each step is progressively refined using retrieved contexts, focusing on accuracy, robustness and logical consistency.\n",
    "    - **Adapted Retrieval**:\n",
    "      - Retrieval is tailored to specific PDF sections and multimodal content rather than broader external corpora.\n",
    "    - **Batch Multimodal Processing**:\n",
    "      - Optimized for handling extracted document images and text in manageable batches.\n",
    "\n",
    "2. **ColQwen Integration**:\n",
    "    - A **Retrieval-Augmented Generation (RAG)** model based on **Qwen2-VL-7B-Instruct**.\n",
    "    - Uses **ColBERT-inspired multi-vector representation** for efficient search and retrieval.\n",
    "    - Capable of processing both text and visual inputs, ensuring thorough analysis of multimodal documents.\n",
    "\n",
    "## Pipeline Description:\n",
    "1. **Document Indexing**:\n",
    "    - The input PDF is indexed using the ColQwen framework for efficient query-based retrieval.\n",
    "2. **Initial Reasoning**:\n",
    "    - A zero-shot CoT reasoning step drafts an initial thought process for answering the query.\n",
    "3. **Stepwise Refinement**:\n",
    "    - Each reasoning step is revised using retrieved context, incorporating relevant data from the indexed document.\n",
    "    - The process emphasizes precision, especially for numerical data, by validating numbers and their contextual alignment.\n",
    "4. **Multimodal Analysis**:\n",
    "    - Extracted relevant pages from the PDF are converted to images for multimodal input processing.\n",
    "    - Text and images are analyzed jointly using Qwen2VL's capabilities.\n",
    "5. **Final Answer Generation**:\n",
    "    - Consolidates all refined reasoning steps into a coherent, contextually accurate response.\n",
    "6. **Answer Validation**:\n",
    "    - Verifies the output to ensure alignment with the query, focusing on accuracy and completeness.\n",
    "\n",
    "\n",
    "## Adaptation Highlights:\n",
    "- **Iterative, Task-Specific Retrieval**:\n",
    "    - Unlike RAT's broad external knowledge retrieval, this implementation focuses on refining reasoning within the scope of a single document.\n",
    "- **Multimodal Focus**:\n",
    "    - Specifically tailored to PDFs with both text and visual data, leveraging Vision-Language models for deeper analysis.\n",
    "- **Simplified Scope**:\n",
    "    - This adaptation prioritizes document-specific insights, making it suitable for tasks like financial report analysis or technical documentation reviews.\n",
    "\n",
    "By taking inspiration from RAT while adapting its principles for document-specific multimodal reasoning, this implementation strikes a balance between theoretical rigor and practical application.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Our Pipeline](./pipeline_rat_colqwen.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwGXD0zyBRqz"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get update\n",
    "!apt-get install poppler-utils\n",
    "\n",
    "from byaldi import RAGMultiModalModel\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "from pdf2image import convert_from_path\n",
    "from groq import Groq\n",
    "import os\n",
    "import PyPDF2\n",
    "from typing import List, Dict\n",
    "\n",
    "# init groq api\n",
    "os.environ[\"GROQ_API_KEY\"] = \"yourGroqAPIToken\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyx2UbGCBRlk"
   },
   "outputs": [],
   "source": [
    "# init models\n",
    "RAG = RAGMultiModalModel.from_pretrained(\"vidore/colqwen2-v1.0\")\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5mxao6w5t08"
   },
   "outputs": [],
   "source": [
    "def extract_multiple_pages(pdf_path: str, results: List[Dict], k: int = 4) -> tuple:\n",
    "    \"\"\" Extracts k best pages into a new PDF \"\"\"\n",
    "\n",
    "    pages_to_extract = [result[\"page_num\"] - 1 for result in results[:k]]\n",
    "\n",
    "    writer = PyPDF2.PdfWriter()\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page_num in pages_to_extract:\n",
    "            writer.add_page(reader.pages[page_num])\n",
    "\n",
    "    output_pdf_path = \"/content/extracted_pages.pdf\"\n",
    "\n",
    "    with open(output_pdf_path, \"wb\") as output_file:\n",
    "        writer.write(output_file)\n",
    "\n",
    "    return output_pdf_path, pages_to_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ZwljPHr5tw0"
   },
   "outputs": [],
   "source": [
    "def create_rat_prompt(query: str, step: int, thoughts: List[str], image_index: int) -> str:\n",
    "    \"\"\" Creates a prompt for RAT reasoning specialized in information retrieval\"\"\"\n",
    "\n",
    "    return f\"\"\"You are an expert at finding and verifying specific information in documents.\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Previous findings:\n",
    "    {' '.join(thoughts[:step])}\n",
    "\n",
    "    Current step {step + 1}, examining image {image_index + 1}:\n",
    "    Let's carefully analyze this page:\n",
    "    1. What specific numbers/data are we looking for?\n",
    "    2. Are we looking for a percentage (growth, change, margin, ...) ? an amount ? a date ?\n",
    "    3. What exact numbers do you see in this image?\n",
    "    4. What is the precise context of each number found? For EACH number found, give the context surrounding EACH of them\n",
    "    5. How does this information connect to our query?\n",
    "\n",
    "    ### Important Guidelines:\n",
    "    - Focus on context and the explicit connection between numbers and the query.\n",
    "    - The frequency of a number being found is NOT a reliable criterion for correctness.\n",
    "    - Verify if the numbers are associated with clear labels (e.g., \"change\", \"margin\", \"growth\").\n",
    "    - For changes (e.g., \"from X to Y\"), ensure both values (start and end) are included in the response.\n",
    "    - Provide a single verified answer, ensuring all alternative numbers are justified or discarded explicitly based on context.\n",
    "\n",
    "    Provide a clear finding that:\n",
    "    - States the exact number found with its full context\n",
    "    - Explains why this number is chosen as the most precise\n",
    "    - Includes additional context (e.g., source section, exact table row or phrase) to justify the choice\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3b3G4eCDBpSt"
   },
   "outputs": [],
   "source": [
    "def process_thought(model, processor, thought: str, step_prompt: str, image_batch) -> List[str]:\n",
    "    \"\"\" Processes a batch of thoughts with the model \"\"\"\n",
    "\n",
    "    messages = []\n",
    "    for image in image_batch:\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": step_prompt\n",
    "        })\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": thought}\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    generate_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=640,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    return processor.batch_decode(\n",
    "        [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generate_ids)],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "\n",
    "def generate_initial_thoughts(text_query: str, model, processor) -> List[str]:\n",
    "    \"\"\" Generates the initial strategy for information retrieval \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"Let's plan our search for this information step by step:\n",
    "\n",
    "    Question: {text_query}\n",
    "\n",
    "    Step 1: First, we need to identify the exact information we're looking for.\n",
    "    Step 2: Then, based on the Question, identify the type of information we want to find : percentage (growth, change, margin, ...), amount, date, etc.\n",
    "    Step 3: Then, locate where this type of information is typically presented.\n",
    "    Step 4: Finally, find any additional context that could help verify our findings. For changes (e.g., \"from X to Y\"), ensure both the starting and ending values are captured along with the calculated change. Avoid assuming correctness based solely on repetition; focus on contextual accuracy.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\",\n",
    "                 \"content\": [\n",
    "                     {\"type\": \"text\",\n",
    "                      \"text\": prompt}\n",
    "                     ]\n",
    "                 }\n",
    "               ]\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(text=[text], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    generate_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=640,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    initial_thoughts = processor.batch_decode(\n",
    "        [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generate_ids)],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "\n",
    "    initial_thoughts_list = [t.strip() for t in initial_thoughts.split(\"\\n\") if t.strip() and \"Step\" in t]\n",
    "\n",
    "    # print initial sequence of thoughts\n",
    "    print(\"\\nGenerated Initial Thoughts:\")\n",
    "    for thought in initial_thoughts_list:\n",
    "        print(thought)\n",
    "\n",
    "    return initial_thoughts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JaJFDDVUBpP2"
   },
   "outputs": [],
   "source": [
    "def rat_process_query(text_query: str, RAG, model, processor, pdf_path: str, k: int = 4) -> str:\n",
    "    \"\"\" RAT-inspired process and final prompt\"\"\"\n",
    "\n",
    "    # init sequence of thoughts\n",
    "    initial_thoughts = generate_initial_thoughts(text_query, model, processor)\n",
    "    revised_thoughts = [initial_thoughts[0]]\n",
    "\n",
    "    # context : extract pages\n",
    "    results = RAG.search(text_query, k=k)\n",
    "    output_pdf_path, page_numbers = extract_multiple_pages(pdf_path, results, k)\n",
    "    images = convert_from_path(output_pdf_path)\n",
    "\n",
    "    # optimize inference, batch images\n",
    "    batch_size = 2\n",
    "    for i in range(1, len(initial_thoughts)):\n",
    "        current_context = \" \".join(revised_thoughts)\n",
    "        current_thought = initial_thoughts[i]\n",
    "\n",
    "        for batch_start in range(0, len(images), batch_size):\n",
    "            image_batch = images[batch_start:batch_start + batch_size]\n",
    "            step_prompt = create_rat_prompt(text_query, i, revised_thoughts, batch_start)\n",
    "            revised_thought = process_thought(model, processor, current_thought, step_prompt, image_batch)\n",
    "            revised_thoughts.extend(revised_thought)\n",
    "\n",
    "    final_prompt = f\"\"\"Based on our systematic information search:\n",
    "\n",
    "    Found information:\n",
    "    {' '.join(revised_thoughts)}\n",
    "\n",
    "    Original question: {text_query}\n",
    "\n",
    "    Please provide:\n",
    "\n",
    "    - NUMBERS FOUND:\n",
    "    [List all relevant numbers with their exact context and source]\n",
    "\n",
    "    - VERIFICATION:\n",
    "    [Explain which number is the most precise and correct, why others are not, and include starting/ending values for changes if relevant. Avoid assuming correctness based solely on repetition. Focus instead on the detailed context and alignment with the query.]\n",
    "\n",
    "    - FINAL ANSWER:\n",
    "    [Provide only the single, verified number that answers the query with its full context. Include changes as \"from X to Y\" if applicable.]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": final_prompt}\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(text=[text], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    generate_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    final_response = processor.batch_decode(\n",
    "        [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generate_ids)],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "\n",
    "    print(\"\\nFinal Response:\")\n",
    "    print(final_response)\n",
    "\n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCBagO6SBpK4"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    pdf_path = \"/your/pdf.pdf\"\n",
    "\n",
    "    # doc indexing\n",
    "    print(\"Indexing PDF...\")\n",
    "\n",
    "    RAG.index(\n",
    "        input_path=pdf_path,\n",
    "        index_name=\"multimodal_rag\",\n",
    "        store_collection_with_index=False,\n",
    "        overwrite=True\n",
    "    )\n",
    "\n",
    "    # querying\n",
    "    query = \"your query\"\n",
    "    response = rat_process_query(query, RAG, model, processor, pdf_path, k=4)\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\\nResponse:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FTCbDZqBpI8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
