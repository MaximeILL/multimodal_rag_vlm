{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdiR2Ww8UOss"
   },
   "source": [
    "# ColPali + Openparse: Multimodal RAG for Q&A\n",
    "\n",
    "This code implements a multimodal Retrieval-Augmented Generation (RAG) system for answering questions about PDF documents. It combines several key technologies :\n",
    "\n",
    "*ColPali*: A pretrained multimodal RAG model based on VLMs  \n",
    "*OpenParse*: For text and table parsing from PDFs  \n",
    "*Qwen2-VL*: A multimodal language model for image analysis  \n",
    "*Groq*: For inference acceleration  \n",
    "\n",
    "*Processing Pipeline:*\n",
    "\n",
    "1. PDF document loading and indexing  \n",
    "2. Relevant passage retrieval  \n",
    "3. Best page extraction and image conversion  \n",
    "4. Parsing with OpenParse:  \n",
    "   - Table structure recognition  \n",
    "   - Cell content extraction  \n",
    "   - Text layout analysis    \n",
    "   - Data formatting and cleaning    \n",
    "5. System prompt generation  \n",
    "6. Response generation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eHkCjv1Qwlr"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get update\n",
    "!apt-get install poppler-utils\n",
    "\n",
    "from byaldi import RAGMultiModalModel\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "from pdf2image import convert_from_path\n",
    "from groq import Groq\n",
    "import base64\n",
    "import os\n",
    "import PyPDF2\n",
    "import openparse\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"yourGroqApiToken\"\n",
    "\n",
    "RAG = RAGMultiModalModel.from_pretrained(\"vidore/colqwen2-v1.0\")\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "                                                        torch_dtype=torch.bfloat16,\n",
    "                                                        attn_implementation=\"flash_attention_2\",\n",
    "                                                        device_map=\"cuda\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "\n",
    "pdf_path = \"/your/pdf.pdf\"\n",
    "\n",
    "RAG.index(input_path=pdf_path,\n",
    "          index_name=\"multimodal_rag\",\n",
    "          store_collection_with_index=False,\n",
    "          overwrite=True,)\n",
    "\n",
    "text_query = \"your query\"\n",
    "results = RAG.search(text_query,k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dky6yz7VQrqb"
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NOkD14IZPa81"
   },
   "outputs": [],
   "source": [
    "images = convert_from_path(pdf_path)\n",
    "image_index = results[0][\"page_num\"] -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wGnTKCtZQpQ-"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image,display\n",
    "display(images[image_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOmGDecdQ1xx"
   },
   "outputs": [],
   "source": [
    "retrieved_page_index = results[0][\"page_num\"] - 1\n",
    "\n",
    "with open(pdf_path, \"rb\") as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    page = reader.pages[retrieved_page_index]\n",
    "\n",
    "    writer = PyPDF2.PdfWriter()\n",
    "    writer.add_page(page)\n",
    "\n",
    "    output_pdf_path = \"/content/extracted_page.pdf\"\n",
    "    with open(output_pdf_path, \"wb\") as output_file:\n",
    "        writer.write(output_file)\n",
    "print(f\"La page {retrieved_page_index + 1} a été extraite dans {output_pdf_path}\")\n",
    "\n",
    "doc_with_tables_path = \"/content/extracted_page.pdf\"\n",
    "\n",
    "parser = openparse.DocumentParser(\n",
    "    table_args={\n",
    "        \"parsing_algorithm\": \"table-transformers\", \n",
    "        \"table_output_format\": \"markdown\"\n",
    "    }\n",
    ")\n",
    "parsed_doc2 = parser.parse(doc_with_tables_path)\n",
    "\n",
    "pdf = openparse.Pdf(doc_with_tables_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlR0eetcQiSf"
   },
   "outputs": [],
   "source": [
    "pdf.display_with_bboxes(\n",
    "    parsed_doc2.nodes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49BOdkDmQfeE"
   },
   "outputs": [],
   "source": [
    "def create_system_prompt(text_query, parsed_content):\n",
    "    return f\"\"\"You are a document analysis assistant specialized in multimodal understanding. Your task is to answer questions accurately using the provided information.\n",
    "\n",
    "    Question: {text_query}\n",
    "\n",
    "    You have access to two information sources:\n",
    "    1. Document image\n",
    "    2. Extracted text content:\n",
    "    {parsed_content}\n",
    "\n",
    "    ANALYSIS PRIORITY:\n",
    "\n",
    "    For CHARTS & VISUALIZATIONS:\n",
    "    - Primary focus on the image input\n",
    "    - Analyze curves, diagrams, and visual elements in detail\n",
    "    - Use image-based reasoning for quantitative information\n",
    "\n",
    "    For TABLES & TEXT:\n",
    "    - Primary focus on the parsed text for structured data\n",
    "    - Reference the image only for:\n",
    "    * Layout verification\n",
    "    * Table structure comprehension\n",
    "    * Visual verification of unclear parsed content\n",
    "\n",
    "    Guidelines:\n",
    "    - First identify the main content type (chart/table/text)\n",
    "    - Use Qwen2VL's visual understanding capabilities for complex charts\n",
    "    - Maintain numerical precision from the correct source\n",
    "    - Cross-reference between sources when necessary\n",
    "    - If switching primary sources, briefly explain why\n",
    "\n",
    "    Now answer the question using the most appropriate source based on content type.\"\"\"\n",
    "\n",
    "\n",
    "def get_parsed_page_content(pdf_path):\n",
    "    try:\n",
    "        parser = openparse.DocumentParser(\n",
    "            table_args={\n",
    "                \"parsing_algorithm\": \"table-transformers\",\n",
    "                \"table_output_format\": \"markdown\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        parsed_doc = parser.parse(pdf_path)\n",
    "\n",
    "        content_parts = []\n",
    "        for node in parsed_doc.nodes:\n",
    "            if hasattr(node, 'text'):\n",
    "                content_parts.append(f\"{node.text}\\n\")\n",
    "\n",
    "        return \"\\n\".join(content_parts)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing PDF: {e}\")\n",
    "        return \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qqdJZ051OUCi"
   },
   "outputs": [],
   "source": [
    "def process_query(text_query, RAG, model, processor, pdf_path):\n",
    "    results = RAG.search(text_query, k=2)\n",
    "    image_index = results[0][\"page_num\"] - 1\n",
    "\n",
    "    images = convert_from_path(pdf_path)\n",
    "\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        page = reader.pages[image_index]\n",
    "        writer = PyPDF2.PdfWriter()\n",
    "        writer.add_page(page)\n",
    "\n",
    "        output_pdf_path = \"/content/extracted_page.pdf\"\n",
    "        with open(output_pdf_path, \"wb\") as output_file:\n",
    "            writer.write(output_file)\n",
    "\n",
    "    parsed_content = get_parsed_page_content(output_pdf_path)\n",
    "    system_prompt = create_system_prompt(text_query, parsed_content)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": images[image_index]},\n",
    "                {\"type\": \"text\", \"text\": text_query}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(text=[text],\n",
    "                      images=image_inputs,\n",
    "                      videos=video_inputs,\n",
    "                      padding=True,\n",
    "                      return_tensors=\"pt\")\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    generate_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generate_ids)\n",
    "    ]\n",
    "\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "\n",
    "    return output_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgG62go_sm6w"
   },
   "outputs": [],
   "source": [
    "response = process_query(text_query, RAG, model, processor, pdf_path)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
